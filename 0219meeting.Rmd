---
title: "0219_meeting0_summary"
author: "Xinyao Wu"
date: "2/19/2020"
output: 
  html_document: 
    number_sections: false
    code_folding: hide
    toc: true
    toc_float: true
---

```{r include=FALSE}
library(tidyverse)
library(MASS)
#sample size
size = 825
nsim = 100
nboost = 100
set.seed(1234)
```

#Review & Summarize
###1.	What is positivity violation?

To identify the Actual causal Effect(ACE), 4 assumptions need to be satisfied, and positivity is one of them. It means if __P(A=a|C=c)is positive for all C such that P(C=c) is nonzero__ does not hold, ACE cannot be identified so it is meaningless to calculate the "formula" ACE.

###2.	What is the difference between $E[Y_{e_(75th)} - Y_{e^*_25th}]$ and $E[Y_{g(A)}-Y_{g(A)^*}]$

Since it is a multi-exporsure analysis, we specify it as one metal first.
$E[Y_{e_(75th)} - Y_{e^*_25th}]$ means using descriptive characteristics (eg.75 percentile,25 percentile) to estimate the causal effect.
$E[Y_{g(A)}-Y_{g(A)^*}]$ means using different distributions when estimating the causal effect.

###3.	What does E[A|C] mean? Why 

It means the average of A under the condition of C. It also means for this project we generate C first, specify it as a known factor. For generating A, the random effect would not come until the average has been generated. 

###4.	How to generate E[A|C]?



$$
E[A|C]=\left\{
\begin{aligned}

 \beta_0^{A_1}+\beta_2^{A_1}*C \\
 \beta_0^{A_2}+\beta_2^{A_2}*C \\
 \beta_0^{A_3}+\beta_2^{A_3}*C 

\end{aligned}
\right.
$$


*Notify that there is a correlation matrix in the paper need to be applied*


###5.	What is the DAG of this study after simplifying?


###6.	Whatâ€™s the structure relationship between sample size, simulation size and bootstrap size ?

*The difference between bootstrap and simulation*

Bootstrap is a nun-parametric way for simulating from an unknown distribution. 
The reason why this project needs both bootstrap and simulation is that we use simulation to generate data, then use bootstrap to sample from 'observed data' to do the checking things(like accuracy, misclassification rate or ect.)

###7.	What does it means to utilize linear regression to fit E[Y|A,C]?

$$E[Y|A,C] = \beta_0 +\beta_1A+\beta_2C$$
only when Assuming A and C are independent, the formula above can hold.
If we pretend not knowing the generation process and assume A and C are independent, this process would be the first step for a normal analysis pipeline. So we do this step to get a general idea of the effect from falsl modeling.

###8.	What is Marginal Structure Model? 

$$E[Y_a]=\beta_0+\beta_1A$$
Above is MSM

To let this formula make sense, each observed point needs to be weighted according to the joint distribution of A and C.

###9.	How to fit MSM? What will be solved after fitting MSM?
 
(1) Calculate IPTW(inverse probability of treatment weights)

(2) Weight each data point

(3) Build marginal model

After fitting MSM, we will get another estimated causal effect. This one should be more closed to the truth because it does not depend on the assumption of independence between A and C. If it is not, we need to check the reason.

#Coding & Tasks

##1.Objective

###(1)specify a case for showing $E[Y_{g(A)}-Y_{g(A)^*}]$

referring 2 in the review:
```{r}

```

###(2)Simulation

```{r simulation}
#confoundings
age = rnorm(825,22.9,4.2)
iq = rnorm(825,26,5)
edu = rbinom(825,1,0.24)
smoke = rbinom(825,1,0.24)

#exposure
#coefficients for three mental
as.beta = c(1,0.3,2,5,8)/50
mn.beta = c(8,9,0.2,3,4)/25
pb.beta = c(0.5,7,2,4,9)/100
beta = data.frame(as.beta,mn.beta,pb.beta) %>% t()
colnames(beta) = c('beta0','beta_age','beta_iq','beta_edu','beta_smoke')
beta 

#calculate the average of confounding 
avg.c = c(1,mean(age),mean(iq),mean(edu),mean(smoke)) 
#calculate the mean exposure
mu = c(sum(avg.c*as.beta),sum(avg.c*mn.beta),sum(avg.c*pb.beta)) %>% as.vector()
names(mu) = c('as','mn','pb')
mu
#variance
corr = matrix(data = c(1,0.59,0.29,0.59,1,0.53,0.29,0.53,1),ncol =3,nrow = 3)
#var of As,Mn,Pb is 2.2,2.6,1.9
var = corr*2.2*2.6*1.9
z = mvrnorm(size,mu,Sigma = var)
as = z[,1]
mn = z[,2]
pb = z[,3]

#outcome
#coef 
coef = c(0.4,1,2.5,0.59,0.31,5.4,2.8,1.8)/3
avg.ec= c(1,mean(as),mean(mn),mean(pb),mean(age),mean(iq),mean(edu),mean(smoke)) 
avg.cs = sum(coef*avg.ec)
names(avg.cs) = "average CS"
avg.cs
#generate the outcome
cs = rnorm(825,avg.ec,35)
```

```{r plot}
#package all into a df
data = data.frame(as,mn,pb,age,iq,edu,smoke,cs)
##write.csv(data,file = 'simulation.csv',row.names = FALSE)
conf = colnames(data)[4:7]
exp = colnames(data)[1:3]

#plot distribuctions of Confoundings
par(mfrow=c(1,length(conf)))
for(i in 1:length(conf)){
  plot(density(data[,conf[i]]),main = conf[i] )
}

#plot distribuctions of Exposure
par(mfrow=c(1,length(exp)))
for(i in 1:length(exp)){
  plot(density(data[,exp[i]]),main = exp[i] )
}
par(mfrow=c(1,1))
plot(density(cs),main = 'cs')
```


###(3)Plot the relationship between Y and the deviation of exposure




###(4)Linear Regression
```{r}
fit1 = lm(cs~as+mn+pb+age+iq+as.factor(smoke)+as.factor(edu),data = data)
summary(fit1)
```

The causal effect would be discribed as :\

(1)with one unit increasing in As, the cs will increase by 0.486 adjusting for other covariants.\
(2)with one unit increasing in Mn, the cs will decrease by 0.6104 adjusting for other covariants.\
(3)with one unit increasing in Pb, the cs will increase by 0.1963 adjusting for other covariants.\

The pre-setting coef for As, Mn, Pb is 0.1333333, 0.3333333, 0.8333333\
Linear regression has a hugh bias when estimating the causal effect.


###(5)MSM



##2.Explanation
__A__: exposure, denoted by __As,Pb,Mn__ \
__C__: observed confoundings, denoted by __age,iq,edu,smoke__ \
__Y__: outcome, denoted by __cs__ \
sample size = 825 \
n_simulation =100 \
n_bootstrap = 100 \

*The distributions of A and C are designed based on paper* \
$$[As,Mn,Pb] \sim MVN(u,\sum)$$

$$IQ \sim N(825,26,5)$$
$$Age \sim N(825,22.9,4.2)$$
$$Education \sim Bin(825,1,0.24)$$
$$Smoke\ Statue \sim Bin(825,1,0.24)$$

$$ CS \sim N(avg.cs,35)$$

#Problems:

1.I am not quite sure the simulation is used on the right way. I have no idea which part needs to be simulated since when you generate the data, you use the distribution function, which can be seemed simulation. 



##multivariant propensity score (literature)




